# 5.找出重复最多的数字

假设有一个10 G 的文件，你电脑只有 2 G 的内存，找出重复最多的数字



这是一个经典的“外部排序/处理”问题，因为数据量远大于内存。解决思路通常是“分而治之”：将大文件分成小块，每块可以在内存中处理，然后合并结果。

对于“找出重复最多的数字”，最常用的方法是基于哈希分桶（Hash Partitioning）或外部排序。这里我们主要讨论哈希分桶，因为它更直接针对频率统计。

**方法：哈希分桶 (Hash Partitioning / External Hashing)**

这个方法分为两个主要阶段：

1. **分区阶段 (Partitioning Phase):**

   - **目标：** 将10GB的大文件分割成多个小文件（桶），使得同一个数字的所有实例都保证落在同一个小文件中。并且每个小文件的大小应该能够让其内部的数字频率统计在2GB内存内完成。

   - 步骤：

     1. 确定桶的数量 (N):

         假设数字是整数。我们需要选择一个合适的桶数量 N。理想情况下，每个桶的数据量加上处理该桶时哈希表（用于计数）所占的内存不应超过2GB。 

        - 如果10GB文件分成了 `N` 个桶，平均每个桶的大小是 `10GB / N`。
        - 假设我们预留1GB给哈希表和其他开销，那么每个桶的原始数据大小最好不要超过1GB。所以 `N` 可以选择为10到20之间，比如 `N = 16` 或 `N = 20`。

     2. **哈希函数：** 选择一个哈希函数。对于整数 `X`，一个简单的哈希函数可以是 `H(X) = X % N`。这意味着数字 `X` 会被分配到第 `X % N` 个桶中。

     3. 处理大文件：

        - 打开10GB的输入文件。
        - 创建 `N` 个临时的输出文件（桶文件），例如 `bucket_0.tmp`, `bucket_1.tmp`, ..., `bucket_N-1.tmp`。为每个输出文件分配一个缓冲区以提高写入效率。
        - 逐个读取大文件中的数字。对于每个数字 `num`： 
          - 计算它应该属于的桶：`bucket_index = num % N` (或其他哈希函数)。
          - 将 `num` 写入到对应的 `bucket_bucket_index.tmp` 文件中。
        - 当所有数字处理完毕后，关闭所有文件。

2. **计数与合并阶段 (Counting and Merging Phase):**

   - **目标：** 找出全局重复次数最多的数字。

   - 步骤：

     1. 初始化全局最大频率 `global_max_freq = 0` 和对应的数字 `most_frequent_num = null`。

     2. 逐个处理桶文件：

         对于每个桶文件 bucket_i.tmp (从 i = 0到 N-1)： 

        - 创建一个内存中的哈希表（例如Python的 `dict` 或Java的 `HashMap`）`local_freq_map` 来统计当前桶内数字的频率。
        - 读取 `bucket_i.tmp` 中的每一个数字 `num`。
        - 更新 `local_freq_map[num] = local_freq_map.get(num, 0) + 1`。
        - 当桶内所有数字处理完毕后，遍历 local_freq_map： 
          - 对于每个 `(num, freq)` 对，如果 `freq > global_max_freq`，则更新 `global_max_freq = freq` 和 `most_frequent_num = num`。
          - （如果需要处理频率相同的情况，比如取较小的数字，可以在此加入逻辑）。
        - 清空 `local_freq_map` 以释放内存，为处理下一个桶做准备。

     3. 当所有桶文件都处理完毕后，`most_frequent_num` 就是重复次数最多的数字，其频率为 `global_max_freq`。

   - **清理：** 删除所有临时的桶文件。

**内存考虑：**

- **分区阶段：** 主要内存消耗是输入文件的读取缓冲区和 `N` 个输出文件的写入缓冲区。例如，每个缓冲区分配几MB，总共可能几十到几百MB，远小于2GB。

- 计数阶段：

   对于每个桶，我们需要将桶内所有数字的频率存储在一个哈希表中。 

  - 假设一个桶文件大小为 `S_bucket`。
  - 哈希表中每个条目需要存储数字本身和它的计数值。例如，一个64位整数（8字节）和其计数值（比如也用8字节，以防溢出），再加上哈希表的额外开销（指针、负载因子等），每个条目可能需要24-32字节。
  - 如果一个桶中有 `D` 个不同的数字，那么哈希表大约需要 `D * (sizeof_number + sizeof_count + overhead_per_entry)` 字节。
  - 我们选择 `N` 的时候就要确保，即使在最坏情况下（一个桶中不同数字很多，但没有一个数字占据主导地位），这个哈希表也能放入2GB内存。如果一个桶内大部分都是重复的同一个数字，那么不同数字 `D` 的数量会很小，哈希表也会很小。
  - 如果经过第一次分桶后，发现某个桶 `bucket_j.tmp` 依然太大，以至于其内部的不同数字的频率统计无法在2GB内存中完成，可以对这个 `bucket_j.tmp` 文件再进行一次或多次哈希分桶（使用不同的哈希函数或更大的 `N'`）。

**数字类型和范围：**

- 如果数字是字符串，哈希函数需要相应调整，内存占用也会变大。
- 如果数字的范围非常大且分布不均，简单的 `X % N` 可能导致数据倾斜（某些桶特别大）。可以考虑使用更通用的哈希函数（如MurmurHash）的结果再取模。

**替代方案：外部排序 (External Sort)**

1. 对10GB文件进行外部排序。
2. 排序后，所有相同的数字会连续排列在一起。
3. 顺序扫描排序后的大文件，很容易统计出每个数字的频率，并找出频率最高的那个。

外部排序本身也是一个复杂的过程，通常涉及生成多个已排序的初始顺串（runs），然后多路归并这些顺串。对于仅仅找出最频繁数字的问题，哈希分桶通常更直接和高效。

**总结：**

哈希分桶方法是最适合这个问题的策略之一。它通过将问题分解为可在内存中处理的小块，有效地绕过了内存限制。关键在于合理选择桶的数量和哈希函数，以确保后续的计数阶段能够顺利进行。